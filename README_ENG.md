# e2e-coref-pytorch

Pytorch implementation of end-to-end Reference Resolution based on bert, model architecture and tensorflow implementation reference [bert-coref](https://github.com/mandarjoshi90/coref).


## Directory Structure

+ data/: Data directory, used to store train/test/val, checkpoint
+ config.py: Configuration file
+ evaluate.py: Test file
+ metrics.py: Evaluation index file
+ model.py: Model file
+ onf_to_data.py: Data processing file, convert the data to the required format
+ predict.py: Predict file
+ tools.py: Tools file
+ train.py: Train file


## Configuration requirements

### Hardware requirements

The program requires more than 16G of memory/video memory, you can modify the max_training_sentences parameter in config.py to reduce the memory usage during training.

### python dependency package

+ python=3.6.6
+ torch=1.5.0
+ transformers=3.0.2
+ numpy=1.18.1


## Dataset collection and processing

### Use ontonotes database


1. Download the ontonotes dataset from the LDC website and modify the "ontonotes_root_dir" configuration in config.py. If it is referred to in English, you can modify the configuration to "/path/to/ontonotes/data/files/data/English/annotations".
2. Check whether train_list/test_list/val_list exists in the data/ directory
3. run `python onf_to_data.py`
4. If there is no problem, three files train.json/test.json/val.json will be generated under data/
> note: train_list/test_list/val_list should be linked to a specific Chinese or English list


### Use your own dataset

Split the data into train.json/test.json/val.json as needed.

The format of the three json files：each line represents a document and each line is a json object.
The specific content of the json object (Note: The json object is formatted into multiple lines in order to be beautiful, and it needs to be 1 line in the real file):

```
{
    "sentences": [["token1", "token2", ...], ...],
    "clusters": [[[sloc1, eloc1], [sloc2, eloc2], ...], ...],
    "speaker_ids" [["speaker#1", ...], ...]
    "sentence_map": [[0, 0, 0, ..., 3, 3, 3], ...],
    "subtoken_map": [[0, 0, 1, 2, 3, ...], ...],
    "genre": "Doc type",
    "doc_key": "Doc name"
}
```

Note: An element in sentences represents a long sentence, which can be composed of several short sentences. 
The length of the long sentence generated by onf_to_data.py is near `max_seq_length`, and `max_seq_length` can be set in the config.py file; 
Several short sentences that make up a long sentence in sentences are composed of **tokens after tokenize**; 
Clusters is the collection of all the reference chains in the document, which is composed of several reference chains.
A reference chain is composed of several mentions. The position of the mention is expressed as: `[the starting position of the token in the doc, the ending position of the token in the doc]`.

Examples (Note: The json object is formatted into multiple lines in order to be beautiful, and it needs to be 1 line in the real file):

```
{
    "sentences": [["It's", " thundering", ",", " how", "can", "I", "send", "text", "messages", "to", "comfort", "my", "girlfriend", "?", "Send", "it", "to", "her", "when", "it", "thunders", "?"]],
    "clusters": [[[12, 12], [17, 17]]],         # （girlfriend， her）
    "speaker_ids": [["a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b", "b", "b"]],
    "sentence_map": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]],
    "subtoken_map": [[1, 1, 2, 3, 3, 4, 5, 5, 6, 6, 7, 8, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16]],
    "genre": "dummy_genre",
    "doc_key": "dummy_data"
}
```

## Model training

1. Obtain training data conforming to the input format according to "Dataset collection and processing"
2. Configure related parameters in config.py and modify the data part in train.py to the data you want to train
3. run `python train.py`


## Model test

1. Configure the relevant parameters in config.py and modify the data part in evaluate.py to the data you want to test
2. run `python evaluate.py`


## Model prediction

1. Configure related parameters in config.py
2. run `python predict.py`


## Model effect

Use the preset configuration to train 57000 steps, the F1 value of OntoNotes Chinese testset is about 0.63, and the English testset is 0.72.

Due to limited ability, the difference between the effect on the English data set and the paper is 0.02.

## Feedback

If there are errors and suggestions, please correct me and explain. Welcome to star and fork this project.
