# e2e-coref-pytorch

Pytorch implementation of End-to-end Coreference Resolution based on [Transformers](https://github.com/huggingface/transformers), model architecture and tensorflow version refer [bert-coref](https://github.com/mandarjoshi90/coref).


## Directory Structure

+ data/: Data directory, used to store train/test/val, checkpoint
+ config.py: Configuration file
+ evaluate.py: Test file
+ metrics.py: Metrics file
+ model.py: Model file
+ onf_to_data.py: Data processing file, convert Ontonotes onf format to the required format
+ predict.py: Predict file
+ tools.py: Tools file
+ train.py: Train file


## Configuration requirements

### Hardware requirements

The program requires more than 16GB GPU, you can modify the max_training_sentences parameter in config.py to reduce the memory usage during training.

### python dependency package

+ python=3.6.6
+ torch=1.5.0
+ transformers=3.0.2
+ numpy=1.18.1


## Dataset collection and processing

### Use Ontonotes dataset(ONLY TESTED ON CHINESE DATASET)


1. Download the ontonotes dataset from the LDC website and modify the "ontonotes_root_dir" configuration in config.py. If it is referred to in English, you can modify the configuration to "/path/to/ontonotes/data/files/data/English/annotations".
2. Check whether train_list/test_list/val_list exists in the data/ directory. If it does not exist, link to the corresponding file. Such as, `ln -si ./data/list/train_english_list ./data/train_list`
3. run `python onf_to_data.py`
4. If there is no problem, three files train.json/test.json/val.json will be generated under ./data
5. Note: Dataset(English/Chinese/Arabic) could easily transform from [bert-coref](https://github.com/mandarjoshi90/coref) jsonlines.

### Use your own dataset

Split the data into train.json/test.json/val.json as needed.

The format of the three json filesï¼šeach line represents a document and each line is a json object.
An example (Note: The example is formatted into multiple lines to be beautiful, and it needs to be one line in the real data file):

```
{
    "sentences": [["token1", "token2", ...], ...],
    "clusters": [[[sloc1, eloc1], [sloc2, eloc2], ...], ...],
    "speaker_ids" [["speaker#1", ...], ...]
    "sentence_map": [[0, 0, 0, ..., 3, 3, 3], ...],
    "subtoken_map": [[0, 0, 1, 2, 3, ...], ...],
    "genre": "Doc type",
    "doc_key": "Doc name"
}
```

Note: An element in "sentences" represents a long sentence, which is composed of several short sentences. 
The length of the long sentence generated by onf_to_data.py is near `max_seq_length`, and `max_seq_length` can be set in the config.py file; 
Short sentences that compose long sentences are composed of **tokens after tokenize**; 
Clusters is the collection of all the reference chains in the document, which is composed of several reference chains.
A reference chain is composed of several mentions. The position of the mention is expressed as: `[the starting position of the token in the doc, the ending position of the token in the doc]`.

An example (Note: The example is formatted into multiple lines to be beautiful, and it needs to be one line in the real data file):

```
{
    "sentences": [["It's", " thundering", ",", " how", "can", "I", "send", "text", "messages", "to", "comfort", "my", "girlfriend", "?", "Send", "it", "to", "her", "when", "it", "thunders", "?"]],
    "clusters": [[[12, 12], [17, 17]]],         # (girlfriend, her)
    "speaker_ids": [["a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "a", "b", "b", "b", "b", "b", "b", "b", "b"]],
    "sentence_map": [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]],
    "subtoken_map": [[1, 1, 2, 3, 3, 4, 5, 5, 6, 6, 7, 8, 8, 9, 10, 10, 11, 12, 13, 14, 15, 16]],
    "genre": "dummy_genre",
    "doc_key": "dummy_data"
}
```

## Model training

1. Obtain training data conforming to the input format according to "Dataset collection and processing"
2. Configure related parameters in config.py, modify the data part in train.py to the data you want to train and transformer_model_name to the required transformer model
3. run `python train.py`


## Model test

1. Configure the relevant parameters in config.py and modify the data part in evaluate.py to the data you want to test
2. run `python evaluate.py`


## Model prediction

1. Configure related parameters in config.py
2. run `python predict.py`


## Model effect

Use the preset configuration to train 70000 steps, the F1 of OntoNotes Chinese testset is about 0.672, and the F1 of English testset is 0.726.

## Feedback

If there are errors and suggestions, please let me know. Welcome to star and fork this project.
